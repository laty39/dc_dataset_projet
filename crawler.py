import os
import time
import random
import requests
import subprocess
import multiprocessing
import urllib.parse
from bs4 import BeautifulSoup
import webbrowser
import threading

# ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  User-Agent ë¦¬ìŠ¤íŠ¸
USER_AGENTS = [
    # âœ… Android (Chrome, Samsung, Edge, Firefox, Opera)
    "Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-A536N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.196 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-N970F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/20.0 Chrome/110.0.5481.153 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-S908U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/22.0 Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/16.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/15.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 3a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.2151.42 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Edge/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Moto G Power) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; OnePlus 9) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Nokia 7.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; LG G7 ThinQ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Sony Xperia 1 II) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",

    # âœ… iOS (Safari, Chrome, Firefox, Edge, Opera)
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_1_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 15_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.8 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/118.0 Mobile/15E148 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) EdgiOS/119.0.2151.42 Mobile/15E148 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) OPR/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.7 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/118.0.2088.46 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
]     
IMAGE_SAVE_DIR = "C:\\Users\\Master\\Downloads\\rome_mgallary_image"

def download_image(image_url, post_id, index=0, save_dir=IMAGE_SAVE_DIR, max_retries=3, base_name=None):
    """
    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (403: ê±´ë„ˆëœ€, 429: ì¬ì‹œë„)
    base_nameì´ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì´ë¦„ì„ ê¸°ë³¸ìœ¼ë¡œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë‹¤ìš´ë¡œë“œ í›„ íŒŒì¼ í¬ê¸°ê°€ min_size(ê¸°ë³¸ 5KB) ë¯¸ë§Œì´ë©´ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    """
    parsed = urllib.parse.urlparse(image_url)
    filename = os.path.basename(parsed.path)
    name, ext = os.path.splitext(filename)
    # í™•ì¥ìê°€ ì—†ê±°ë‚˜ ì´ë¯¸ì§€ í™•ì¥ìê°€ ì•„ë‹Œ ê²½ìš° .jpgë¡œ ì§€ì •
    if not ext or ext.lower() in ['.php', '.asp', '.aspx']:
        ext = ".jpg"
    if base_name is not None:
        local_filename = f"post_{base_name}_img{index}{ext}"
    else:
        local_filename = f"post_{post_id}_img{index}{ext}"
    local_filepath = os.path.join(save_dir, local_filename)
    
    for attempt in range(max_retries):
        try:
            response = requests.get(
                image_url, stream=True, 
                headers={"User-Agent": random.choice(USER_AGENTS)}
            )
            if response.status_code == 429:
                wait_time = random.uniform(5, 15)
                print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 429 - {wait_time:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„")
                time.sleep(wait_time)
                continue
            if response.status_code == 403:
                print(f"â›” {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 403 - ê±´ë„ˆëœ€")
                return None
            response.raise_for_status()

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            with open(local_filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    
            file_size = os.path.getsize(local_filepath)
            # íŒŒì¼ í¬ê¸° ê²€ì¦: min_sizeì™€ ì¼ì¹˜í•˜ë©´ ì¬ì‹œë„ ëŒ€ìƒ
            if "btn_close02.gif" in image_url or file_size == 574:
                        print(f"â›” {post_id} ë¡œë”© ì¤‘ ì´ë¯¸ì§€(placeholder)ë¡œ íŒë‹¨ë˜ì–´ ê±´ë„ˆëœ€: {image_url}")
                        os.remove(local_filepath)
                        return []
                
            print(f"âœ… {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_filepath}")
            return local_filepath
    
    print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì§€ ëª»í•¨")
    return None
	
def process_comments(soup, post_id):
    """
    DCInside ëŒ“ê¸€ ì˜ì—­(#comment_box)ì—ì„œ ë©”ì¸ ëŒ“ê¸€ê³¼ ë‹µê¸€ì„ ê·¸ë£¹í™”í•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤.
    ëŒ“ê¸€ ë³¸ë¬¸ ë‚´ì˜ ì´ë¯¸ì§€(ë””ì‹œì½˜ ë“±)ê°€ ìˆë‹¤ë©´ ë‹¤ìš´ë¡œë“œí•˜ê³ , srcë¥¼ ë¡œì»¬ ê²½ë¡œë¡œ ëŒ€ì²´í•œ í›„ HTMLì„ ê·¸ëŒ€ë¡œ ë³´ì¡´í•©ë‹ˆë‹¤.
    """
    comments_data = []
    comment_box = soup.find("div", id="comment_box")
    if not comment_box:
        print("ëŒ“ê¸€ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨.")
        return comments_data

    comment_items = comment_box.select("ul.all-comment-lst > li")
    current_main = None

    for li in comment_items:
        txt_tag = li.find("p", class_="txt")
        if txt_tag:
            # ëŒ“ê¸€ ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
            for idx, img in enumerate(txt_tag.find_all("img"), start=1):
                img_url = img.get("data-original") or img.get("src")
                if not img_url:
                    continue
                local_filepath = download_image(img_url, post_id, index=idx, save_dir=IMAGE_SAVE_DIR)
                if local_filepath:
                    img["src"] = local_filepath
                    if "data-original" in img.attrs:
                        del img["data-original"]
                else:
                    print(f"ëŒ“ê¸€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ëŒ€ì‹ , HTML ê·¸ëŒ€ë¡œ ë³´ì¡´
            content = str(txt_tag)
        else:
            content = ""

        nick_tag = li.find("a", class_="nick")
        nickname = nick_tag.get_text(strip=True) if nick_tag else ""
        date_tag = li.find("span", class_="date")
        date_text = date_tag.get_text(strip=True) if date_tag else ""

        comment_info = {
            "nickname": nickname,
            "content": content,
            "date": date_text,
            "html": str(li)
        }
        
        classes = li.get("class", [])
        if "comment" in classes and "comment-add" not in classes:
            current_main = comment_info
            current_main["replies"] = []
            comments_data.append(current_main)
        elif "comment-add" in classes:
            if current_main:
                current_main["replies"].append(comment_info)
            else:
                comments_data.append(comment_info)
        else:
            comments_data.append(comment_info)

    return comments_data


def fetch_dcinside_page(page_number, max_retries=5):
    """ DCInside ê²Œì‹œê¸€ í¬ë¡¤ë§ + 429(ì¬ì‹œë„) + 403(ê±´ë„ˆë›°ê¸°) """
    session = requests.Session()
    session.headers.update({
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://m.dcinside.com/",
    })

    page_url = f"https://m.dcinside.com/board/rome?page={page_number}"
    print(f"ğŸ‘‰ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì¤‘: {page_url}")

    for attempt in range(max_retries):
        try:
            response = session.get(page_url, timeout=10)
            if response.status_code == 429:
                print(f"ğŸš¨ í˜ì´ì§€ {page_number}: 429 Too Many Requests")
                time.sleep(10)
                continue
            response.raise_for_status()
            break
        except Exception as e:
            print(f"í˜ì´ì§€ {page_number} ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            time.sleep(5)
    else:
        print(f"âŒ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì‹¤íŒ¨")
        return []  # ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    soup = BeautifulSoup(response.text, "html.parser")
    posts = soup.select("ul.gall-detail-lst > li:not(.adv-inner)")
    crawled_posts = []

    for post in posts:
        ginfo = post.select_one("ul.ginfo")
        if not ginfo:
            continue
        li_list = ginfo.find_all("li")
    
        category = li_list[0].get_text(strip=True) if li_list else ""
    
        recommend = 0
        if li_list:
            rec_li = li_list[-1]
            span_tag = rec_li.find("span")
            if span_tag:
                try:
                    recommend = int(span_tag.get_text(strip=True))
                except ValueError:
                    recommend = 0

        subjectin = post.select_one("span.subjectin")
        title = subjectin.get_text(strip=True) if subjectin else ""
    
        # ì¡°ê±´
    
        if 'íŒë„' in title:
            a_tag = post.find("a", class_="lt")
            if not a_tag or not a_tag.has_attr("href"):
                continue
            detail_url = a_tag["href"]
            print("â†’ ì¡°ê±´ ì¶©ì¡±, ìƒì„¸ í˜ì´ì§€ URL:", detail_url)

            try:
                detail_resp = session.get(detail_url, timeout=10)
                detail_resp.raise_for_status()
            except Exception as e:
                print("ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", e)
                continue
            
            detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            
            # URLì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ post_idë¥¼ ì¶”ì¶œ (ì¿¼ë¦¬ë¬¸ ì œê±°)
            parsed_detail = urllib.parse.urlparse(detail_url)
            post_id = os.path.basename(parsed_detail.path)

            for idx, img in enumerate(detail_soup.find_all("img"), start=1):
                img_url = img.get("data-original") or img.get("src")
                if not img_url:
                    continue
                local_filepath = download_image(img_url, post_id, index=idx, save_dir=IMAGE_SAVE_DIR)
                if local_filepath:
                    img["src"] = local_filepath
                    if "data-original" in img.attrs:
                        del img["data-original"]
                else:
                    print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")

            content_tag = detail_soup.select_one("div.gall-thum-btm div.thum-txt > div.thum-txtin")
            if content_tag:
                # ê´‘ê³  ê´€ë ¨ íƒœê·¸ ì œê±°: adv-groupno í´ë˜ìŠ¤ë‚˜ script íƒœê·¸ ì œê±°
                for tag in content_tag.find_all(["div", "script"], class_="adv-groupno"):
                    tag.decompose()
                for script in content_tag.find_all("script"):
                    script.decompose()
                content = str(content_tag)
            else:
                content = "(ë³¸ë¬¸ ì—†ìŒ)"
            
            comments_data = process_comments(detail_soup, post_id)
            
            post_data = {
                "post_id": post_id,
                "title": title,
                "content": content,
                "comments": comments_data,
                "recommend": recommend,  
            }
            crawled_posts.append(post_data)
            print(f"âœ… ê²Œì‹œê¸€ {post_id} í¬ë¡¤ë§ ì™„ë£Œ: {title[:20]}")

    return crawled_posts

def generate_html(all_posts, filename="dcinside_crawled_posts.html"):
    """
    ëˆ„ì ëœ ê²Œì‹œê¸€ ì •ë³´ë¥¼ HTML í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    ê° ê²Œì‹œê¸€ì€ ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€(ë©”ì¸ ëŒ“ê¸€ ë° ë‹µê¸€) ìˆœìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
    ëŒ“ê¸€ì˜ ì‘ì„±ì ë‹‰ë„¤ì„ì€ ìµëª…í™”ë˜ì–´ "ã…‡ã…‡1", "ã…‡ã…‡2", ... í˜•ì‹ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.
    """
    # ìµëª… ë‹‰ë„¤ì„ ë§¤í•‘ ì‚¬ì „ê³¼ ì¹´ìš´í„°
    nick_mapping = {}
    next_anonym = 1

    def get_anonym(nickname):
        nonlocal next_anonym
        # ë‹‰ë„¤ì„ì´ ì—†ìœ¼ë©´ "ìµëª…" ì²˜ë¦¬
        if not nickname:
            return "ìµëª…"
        if nickname not in nick_mapping:
            nick_mapping[nickname] = f"ã…‡ã…‡{next_anonym}"
            next_anonym += 1
        return nick_mapping[nickname]

    html = """<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>DCInside í¬ë¡¤ë§ ê²°ê³¼</title>
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; }
        .post { border: 1px solid #ccc; margin: 20px 0; padding: 10px; }
        .post h2 { margin: 0; }
        .comments { margin-top: 10px; padding-left: 20px; }
        .reply { margin-left: 20px; }
    </style>
</head>
<body>
    <h1>DCInside í¬ë¡¤ë§ ê²°ê³¼</h1>
"""
    for post in all_posts:
        recommend = post.get("recommend", 0)
        post_html = f"<div class='post'>\n"
        post_html += f"<h2>{post['title']} (ê²Œì‹œê¸€ ë²ˆí˜¸: {post['post_id']}, ì¶”ì²œìˆ˜: {recommend})</h2>\n"
        post_html += f"<div class='content'><p>{post['content'].replace(chr(10), '<br>')}</p></div>\n"
        
        if post.get("comments"):
            post_html += "<div class='comments'><h3>ëŒ“ê¸€:</h3><ul>\n"
            for comment in post["comments"]:
                # ìµëª…í™”ëœ ë‹‰ë„¤ì„ìœ¼ë¡œ ë³€í™˜
                anon_nick = get_anonym(comment.get("nickname", "ìµëª…"))
                content   = comment.get("content", "")
                post_html += f"<li><strong>{anon_nick}:</strong> {content}</li>\n"
                if comment.get("replies"):
                    post_html += "<ul>\n"
                    for reply in comment["replies"]:
                        rep_anon = get_anonym(reply.get("nickname", "ìµëª…"))
                        rep_content = reply.get("content", "")
                        post_html += f"<li class='reply'><strong>{rep_anon}:</strong> {rep_content}</li>\n"
                    post_html += "</ul>\n"
            post_html += "</ul></div>\n"
        else:
            post_html += "<div class='comments'><p>ëŒ“ê¸€ ì—†ìŒ</p></div>\n"
        post_html += "</div>\n"
        html += post_html

    html += """
</body>
</html>
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"âœ… HTML ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(all_posts)}ê°œ ê²Œì‹œê¸€) â†’ {filename}")

def multiprocess_crawl(start_page, end_page, num_workers):
    """ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ í˜ì´ì§€ ë‹¨ìœ„ í¬ë¡¤ë§ í›„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    page_numbers = list(range(start_page, end_page + 1))
    all_posts = []
    with multiprocessing.Pool(num_workers) as pool:
        # ê° í˜ì´ì§€ì— ëŒ€í•´ fetch_dcinside_pageë¥¼ ì‹¤í–‰
        results = pool.map(fetch_dcinside_page, page_numbers)
    # ê²°ê³¼(ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)ë¥¼ í‰íƒ„í™”
    for page_posts in results:
        all_posts.extend(page_posts)
    return all_posts
        
if __name__ == "__main__":
    multiprocessing.freeze_support()  # Windowsì—ì„œ multiprocess ì•ˆì •í™”
    # ì›í•˜ëŠ” í˜ì´ì§€ ë²”ìœ„ ì„¤ì • (ì˜ˆ: 1í˜ì´ì§€ë¶€í„° 100í˜ì´ì§€ê¹Œì§€)
    start_page = 1
    end_page = 22644
    num_workers = 15
    all_posts = multiprocess_crawl(start_page, end_page, num_workers)
    html_filename = "dcinside_crawled_posts.html"
    generate_html(all_posts, html_filename)
