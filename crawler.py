import os
import time
import random
import requests
import subprocess
import multiprocessing
import urllib.parse
from bs4 import BeautifulSoup
import webbrowser
import threading

# ---------------------------
# ì„¤ì • ë° ìƒìˆ˜
# ---------------------------
USER_AGENTS = [
    # Android (Chrome, Samsung, Edge, Firefox, Opera) ë“± ëª¨ë°”ì¼ UA ë¦¬ìŠ¤íŠ¸
    "Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-A536N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.196 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-N970F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/20.0 Chrome/110.0.5481.153 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-S908U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/22.0 Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/16.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/15.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 3a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.2151.42 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Edge/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Moto G Power) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; OnePlus 9) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Nokia 7.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; LG G7 ThinQ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Sony Xperia 1 II) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
]

IMAGE_SAVE_DIR = os.path.join(os.environ.get("USERPROFILE", "."), "Downloads", "rome_mgallary_image")
OUTPUT_DIR = "output"

# ---------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------
def get_random_headers(referer: str = "https://m.dcinside.com/") -> dict:
    """ëœë¤ User-Agentì™€ ê¸°ë³¸ Refererë¥¼ í¬í•¨í•œ í—¤ë” ë°˜í™˜."""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": referer,
    }

def make_dirs(path: str):
    """ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±."""
    os.makedirs(path, exist_ok=True)

# ---------------------------
# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê´€ë ¨
# ---------------------------
def download_image(image_url: str, post_id: str, index: int = 0, save_dir: str = IMAGE_SAVE_DIR,
                   max_retries: int = 3, base_name: str = None) -> str:
    """
    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜.
    
    - 403 ì˜¤ë¥˜ì¸ ê²½ìš° ê±´ë„ˆë›°ë©°, 429 ì˜¤ë¥˜ì‹œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    - ë‹¤ìš´ë¡œë“œí•œ ì´ë¯¸ì§€ê°€ placeholderì¼ ê²½ìš° ì‚­ì œ í›„ None ë°˜í™˜.
    """
    parsed = urllib.parse.urlparse(image_url)
    filename = os.path.basename(parsed.path)
    name, ext = os.path.splitext(filename)
    if not ext or ext.lower() in ['.php', '.asp', '.aspx']:
        ext = ".jpg"
    local_filename = f"post_{base_name if base_name else post_id}_img{index}{ext}"
    local_filepath = os.path.join(save_dir, local_filename)
    make_dirs(os.path.dirname(local_filepath))

    for attempt in range(max_retries):
        try:
            response = requests.get(
                image_url, stream=True, 
                headers={"User-Agent": random.choice(USER_AGENTS)}
            )
            if response.status_code == 429:
                wait_time = random.uniform(5, 15)
                print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 429 - {wait_time:.1f}s í›„ ì¬ì‹œë„")
                time.sleep(wait_time)
                continue
            if response.status_code == 403:
                print(f"â›” {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 403 - ê±´ë„ˆëœ€")
                return None

            response.raise_for_status()
            with open(local_filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            file_size = os.path.getsize(local_filepath)
            # íŒŒì¼ í¬ê¸°ê°€ íŠ¹ì • í¬ê¸°(ì˜ˆ: 574ë°”ì´íŠ¸)ì¼ ê²½ìš° placeholderë¡œ íŒë‹¨
            if "btn_close02.gif" in image_url or file_size == 574:
                print(f"â›” {post_id} placeholder ì´ë¯¸ì§€ë¡œ íŒë‹¨ë˜ì–´ ê±´ë„ˆëœ€: {image_url}")
                os.remove(local_filepath)
                return None

            print(f"âœ… {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_filepath}")
            return local_filepath

        except requests.exceptions.RequestException as e:
            print(f"âŒ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {image_url}\nì˜¤ë¥˜: {e}")
            time.sleep(0.1)
    print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ìµœì¢… ì‹¤íŒ¨")
    return None

# ---------------------------
# ëŒ“ê¸€ ì²˜ë¦¬ ê´€ë ¨
# ---------------------------
def process_comments(soup: BeautifulSoup, post_id: str) -> list:
    """
    DCInside ëŒ“ê¸€ ì˜ì—­(#comment_box) ë‚´ ëŒ“ê¸€ ë° ë‹µê¸€ íŒŒì‹±.
    
    ëŒ“ê¸€ ë‚´ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¡œì»¬ ê²½ë¡œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    """
    comments_data = []
    comment_box = soup.find("div", id="comment_box")
    if not comment_box:
        print("ëŒ“ê¸€ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨.")
        return comments_data

    comment_items = comment_box.select("ul.all-comment-lst > li")
    current_main = None

    for li in comment_items:
        txt_tag = li.find("p", class_="txt")
        if txt_tag:
            for idx, img in enumerate(txt_tag.find_all("img"), start=1):
                img_url = img.get("data-original") or img.get("src")
                if not img_url:
                    continue
                local_filepath = download_image(img_url, post_id, index=idx, save_dir=IMAGE_SAVE_DIR)
                if local_filepath:
                    img["src"] = local_filepath
                    img.attrs.pop("data-original", None)
                else:
                    print(f"ëŒ“ê¸€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")
            content = str(txt_tag)
        else:
            content = ""

        nickname = li.find("a", class_="nick").get_text(strip=True) if li.find("a", class_="nick") else ""
        date_text = li.find("span", class_="date").get_text(strip=True) if li.find("span", class_="date") else ""

        comment_info = {
            "nickname": nickname,
            "content": content,
            "date": date_text,
            "html": str(li)
        }

        classes = li.get("class", [])
        if "comment" in classes and "comment-add" not in classes:
            current_main = comment_info
            current_main["replies"] = []
            comments_data.append(current_main)
        elif "comment-add" in classes:
            if current_main:
                current_main["replies"].append(comment_info)
            else:
                comments_data.append(comment_info)
        else:
            comments_data.append(comment_info)
    return comments_data

# ---------------------------
# í¬ë¡¤ë§ í•¨ìˆ˜
# ---------------------------
def fetch_dcinside_page(page_number: int, max_retries: int = 5) -> list:
    """
    ì£¼ì–´ì§„ í˜ì´ì§€ ë²ˆí˜¸ì˜ ê²Œì‹œê¸€ë“¤ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    ì¡°ê±´ì— ë§ëŠ” ê²Œì‹œê¸€(ì˜ˆ: 'ì—°ì¬' í¬í•¨)ì¸ ê²½ìš° ìƒì„¸ í˜ì´ì§€ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    session = requests.Session()
    session.headers.update(get_random_headers())

    page_url = f"https://m.dcinside.com/board/rome?page={page_number}"
    print(f"ğŸ‘‰ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì¤‘: {page_url}")

    for attempt in range(max_retries):
        try:
            response = session.get(page_url, timeout=30)
        except requests.exceptions.ReadTimeout as e:
            print(f"ğŸš¨ í˜ì´ì§€ {page_number} ReadTimeout ë°œìƒ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            time.sleep(10)
            continue

        if response.status_code == 429:
            print(f"ğŸš¨ í˜ì´ì§€ {page_number}: 429 Too Many Requests")
            time.sleep(10)
            continue

        try:
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ í˜ì´ì§€ {page_number} ìš”ì²­ ì‹¤íŒ¨: {e}")
            time.sleep(10)
            continue
        break
    else:
        print(f"âŒ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì‹¤íŒ¨")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    posts = soup.select("ul.gall-detail-lst > li:not(.adv-inner)")
    crawled_posts = []

    for post in posts:
        ginfo = post.select_one("ul.ginfo")
        if not ginfo:
            continue

        li_list = ginfo.find_all("li")
        category = li_list[0].get_text(strip=True) if li_list else ""

        # ê¸°ë³¸ ì •ë³´ íŒŒì‹±
        recommend = 0
        nickname = ""
        time_info = ""
        if li_list:
            try:
                recommend = int(li_list[-1].find("span").get_text(strip=True))
            except (ValueError, AttributeError):
                recommend = 0
            if len(li_list) >= 3:
                nickname = li_list[1].get_text(strip=True)
                time_info = li_list[2].get_text(strip=True)

        # 'ì—°ì¬' ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        if 'ì—°ì¬' in category:
            a_tag = post.find("a", class_="lt")
            if not a_tag or not a_tag.has_attr("href"):
                continue
            detail_url = a_tag["href"]
            print("â†’ ì¡°ê±´ ì¶©ì¡±, ìƒì„¸ í˜ì´ì§€ URL:", detail_url)

            try:
                detail_resp = session.get(detail_url, timeout=100)
                detail_resp.raise_for_status()
            except Exception as e:
                print("ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", e)
                continue

            detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            parsed_detail = urllib.parse.urlparse(detail_url)
            post_id = os.path.basename(parsed_detail.path)

            # ìƒì„¸ í˜ì´ì§€ ë‚´ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° src ìˆ˜ì •
            for idx, img in enumerate(detail_soup.find_all("img"), start=1):
                img_url = img.get("data-original") or img.get("src")
                if not img_url:
                    continue
                local_filepath = download_image(img_url, post_id, index=idx, save_dir=IMAGE_SAVE_DIR)
                if local_filepath:
                    img["src"] = local_filepath
                    img.attrs.pop("data-original", None)
                else:
                    print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")

            # ë³¸ë¬¸ íŒŒì‹± (ê´‘ê³  ìŠ¤í¬ë¦½íŠ¸/íƒœê·¸ ì œê±°)
            content_tag = detail_soup.select_one("div.gall-thum-btm div.thum-txt > div.thum-txtin")
            if content_tag:
                for tag in content_tag.find_all(["div", "script"], class_="adv-groupno"):
                    tag.decompose()
                for script in content_tag.find_all("script"):
                    script.decompose()
                content = str(content_tag)
            else:
                content = "(ë³¸ë¬¸ ì—†ìŒ)"

            comments_data = process_comments(detail_soup, post_id)

            post_data = {
                "post_id": post_id,
                "title": post.select_one("span.subjectin").get_text(strip=True) if post.select_one("span.subjectin") else "",
                "content": content,
                "comments": comments_data,
                "recommend": recommend,
                "nickname": nickname,
                "time_info": time_info
            }
            crawled_posts.append(post_data)
            print(f"âœ… ê²Œì‹œê¸€ {post_id} í¬ë¡¤ë§ ì™„ë£Œ: {post_data['title'][:20]}")

    return crawled_posts

# ---------------------------
# HTML ì €ì¥ ê´€ë ¨
# ---------------------------
def generate_html(post: dict, output_dir: str = OUTPUT_DIR):
    """
    ê°œë³„ ê²Œì‹œê¸€ì„ HTML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    make_dirs(output_dir)
    post_id = post.get("post_id", "unknown")
    filename = os.path.join(output_dir, f"post_{post_id}.html")

    recommend = post.get("recommend", 0)
    nickname = post.get("nickname", "ìµëª…")
    time_info = post.get("time_info", "")

    html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>{post['title']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; padding: 20px; }}
        .post {{ border: 1px solid #ccc; padding: 10px; margin-bottom: 20px; }}
        .comments {{ margin-top: 10px; padding-left: 20px; }}
        .reply {{ margin-left: 20px; }}
    </style>
</head>
<body>
    <div class="post">
        <h2>{post['title']} (ì¶”ì²œìˆ˜: {recommend})</h2>
        <p><strong>ê¸€ì“´ì´:</strong> {nickname} | <strong>ì‘ì„± ì‹œê°:</strong> {time_info}</p>
        <div class="content">{post['content']}</div>
    </div>
    <div class="comments">
        <h3>ëŒ“ê¸€</h3>
        <ul>
    """
    for comment in post.get("comments", []):
        comment_nick = comment.get("nickname", "ìµëª…")
        comment_content = comment.get("content", "")
        html_content += f"<li><strong>{comment_nick}:</strong> {comment_content}</li>\n"
        if "replies" in comment:
            html_content += "<ul>\n"
            for reply in comment["replies"]:
                reply_nick = reply.get("nickname", "ìµëª…")
                reply_content = reply.get("content", "")
                html_content += f"<li class='reply'><strong>{reply_nick}:</strong> {reply_content}</li>\n"
            html_content += "</ul>\n"
    html_content += """
        </ul>
    </div>
</body>
</html>
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html_content)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")

def save_all_posts(all_posts: list):
    """ì „ì²´ ê²Œì‹œê¸€ì„ ê°œë³„ HTML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    for post in all_posts:
        generate_html(post)

# ---------------------------
# ë©€í‹°í”„ë¡œì„¸ìŠ¤ í¬ë¡¤ë§
# ---------------------------
def multiprocess_crawl(start_page: int, end_page: int, num_workers: int) -> list:
    """
    ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë‹¨ìœ„ í¬ë¡¤ë§ í›„ ëª¨ë“  ê²Œì‹œê¸€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    page_numbers = list(range(start_page, end_page + 1))
    all_posts = []
    with multiprocessing.Pool(num_workers) as pool:
        results = pool.map(fetch_dcinside_page, page_numbers)
    for page_posts in results:
        if page_posts:
            all_posts.extend(page_posts)
    return all_posts

# ---------------------------
# main í•¨ìˆ˜
# ---------------------------
def main():
    multiprocessing.freeze_support()
    start_page = 1
    end_page = 10  # ì˜ˆì œìš©: 10í˜ì´ì§€
    num_workers = 20

    all_posts = multiprocess_crawl(start_page, end_page, num_workers)
    print(f"ğŸ“‚ ì´ {len(all_posts)}ê°œì˜ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì™„ë£Œ, ì €ì¥ ì‹œì‘...")
    save_all_posts(all_posts)
    print("ğŸ‰ ëª¨ë“  ê²Œì‹œê¸€ ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
