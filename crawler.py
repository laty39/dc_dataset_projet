import os
import time
import random
import requests
import subprocess
import multiprocessing
import urllib.parse
from bs4 import BeautifulSoup
import webbrowser
import threading

# ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  User-Agent ë¦¬ìŠ¤íŠ¸
USER_AGENTS = [
    # âœ… Android (Chrome, Samsung, Edge, Firefox, Opera)
    "Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-A536N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.196 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-N970F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/20.0 Chrome/110.0.5481.153 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-S908U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/22.0 Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/16.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/15.0 Chrome/92.0.4515.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Pixel 3a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.2151.42 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Edge/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Moto G Power) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; OnePlus 9) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.159 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; Nokia 7.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 9; LG G7 ThinQ) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.2088.46 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; Sony Xperia 1 II) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.224 Mobile Safari/537.36",

    # âœ… iOS (Safari, Chrome, Firefox, Edge, Opera)
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_1_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 15_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.8 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/118.0 Mobile/15E148 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) EdgiOS/119.0.2151.42 Mobile/15E148 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) OPR/120.0.6099.224 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.7 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/118.0.2088.46 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.224 Mobile/15E148 Safari/604.1",
]     
IMAGE_SAVE_DIR = "D:\\rome_mgallary_image"

def download_image(image_url, post_id, index=0, save_dir=IMAGE_SAVE_DIR, max_retries=3, base_name=None):
    """
    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (403: ê±´ë„ˆëœ€, 429: ì¬ì‹œë„)
    base_nameì´ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì´ë¦„ì„ ê¸°ë³¸ìœ¼ë¡œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    parsed = urllib.parse.urlparse(image_url)
    filename = os.path.basename(parsed.path)
    name, ext = os.path.splitext(filename)
    # í™•ì¥ìê°€ ì—†ê±°ë‚˜ ì´ë¯¸ì§€ í™•ì¥ìê°€ ì•„ë‹Œ ê²½ìš° .jpgë¡œ ì§€ì •
    if not ext or ext.lower() in ['.php', '.asp', '.aspx']:
        ext = ".jpg"
    if base_name is not None:
        local_filename = f"post_{base_name}_img{index}{ext}"
    else:
        local_filename = f"post_{post_id}_img{index}{ext}"
    local_filepath = os.path.join(save_dir, local_filename)
    
    for attempt in range(max_retries):
        try:
            response = requests.get(image_url, stream=True, headers={"User-Agent": random.choice(USER_AGENTS)})
            if response.status_code == 429:
                wait_time = random.uniform(5, 15)
                print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 429 - {wait_time:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„")
                time.sleep(wait_time)
                continue
            if response.status_code == 403:
                print(f"â›” {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 403 - ê±´ë„ˆëœ€")
                return None
            response.raise_for_status()
            with open(local_filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"âœ… {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_filepath}")
            return local_filepath
        except requests.exceptions.RequestException as e:
            print(f"âŒ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            time.sleep(5)
    
    print(f"ğŸš¨ {post_id} ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì§€ ëª»í•¨")
    return None


def fetch_dcinside_page(page_number, max_retries=5):
    """ DCInside ê²Œì‹œê¸€ í¬ë¡¤ë§ + 429(ì¬ì‹œë„) + 403(ê±´ë„ˆë›°ê¸°) """
    session = requests.Session()
    session.headers.update({
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://m.dcinside.com/",
    })

    page_url = f"https://m.dcinside.com/board/rome?page={page_number}"
    print(f"ğŸ‘‰ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì¤‘: {page_url}")

    for attempt in range(max_retries):
        try:
            response = session.get(page_url, timeout=10)
            if response.status_code == 429:
                print(f"ğŸš¨ í˜ì´ì§€ {page_number}: 429 Too Many Requests")
                time.sleep(10)
                continue
            response.raise_for_status()
            break
        except Exception as e:
            print(f"í˜ì´ì§€ {page_number} ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            time.sleep(5)
    else:
        print(f"âŒ í˜ì´ì§€ {page_number} í¬ë¡¤ë§ ì‹¤íŒ¨")
        return []  # ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    soup = BeautifulSoup(response.text, "html.parser")
    posts = soup.select("ul.gall-detail-lst > li:not(.adv-inner)")
    crawled_posts = []

    for post in posts:
        ginfo = post.select_one("ul.ginfo")
        if not ginfo:
            continue
        li_list = ginfo.find_all("li")
    
        category = li_list[0].get_text(strip=True) if li_list else ""
    
        recommend = 0
        if li_list:
            rec_li = li_list[-1]
            span_tag = rec_li.find("span")
            if span_tag:
                try:
                    recommend = int(span_tag.get_text(strip=True))
                except ValueError:
                    recommend = 0

        subjectin = post.select_one("span.subjectin")
        title = subjectin.get_text(strip=True) if subjectin else ""
    
        # ì¡°ê±´
    
        if category == 'ğŸ“œì—°ì¬':
            a_tag = post.find("a", class_="lt")
            if not a_tag or not a_tag.has_attr("href"):
                continue
            detail_url = a_tag["href"]
            print("â†’ ì¡°ê±´ ì¶©ì¡±, ìƒì„¸ í˜ì´ì§€ URL:", detail_url)

            try:
                detail_resp = session.get(detail_url, timeout=10)
                detail_resp.raise_for_status()
            except Exception as e:
                print("ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", e)
                continue
            
            detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            
            # URLì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ post_idë¥¼ ì¶”ì¶œ (ì¿¼ë¦¬ë¬¸ ì œê±°)
            parsed_detail = urllib.parse.urlparse(detail_url)
            post_id = os.path.basename(parsed_detail.path)

            for idx, img in enumerate(detail_soup.find_all("img"), start=1):
                img_url = img.get("data-original") or img.get("src")
                if not img_url:
                    continue
                local_filepath = download_image(img_url, post_id, index=idx, save_dir=IMAGE_SAVE_DIR)
                if local_filepath:
                    img["src"] = local_filepath
                    if "data-original" in img.attrs:
                        del img["data-original"]
                else:
                    print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")

            content_tag = detail_soup.select_one("div.gall-thum-btm div.thum-txt > div.thum-txtin")
            content = str(content_tag) if content_tag else "(ë³¸ë¬¸ ì—†ìŒ)"
            
            comments = []
            comment_box = detail_soup.select_one("#comment_box")
            if comment_box:
                comment_items = comment_box.select("ul.all-comment-lst li.comment, ul.all-comment-lst li.comment-add")
                for li_comment in comment_items:
                    p_tag = li_comment.select_one("p.txt")
                    if p_tag:
                        comments.append(p_tag.get_text(strip=True))
            
            post_data = {
                "post_id": post_id,
                "title": title,
                "content": content,
                "comments": comments,
                "recommend": recommend,  
            }
            crawled_posts.append(post_data)
            print(f"âœ… ê²Œì‹œê¸€ {post_id} í¬ë¡¤ë§ ì™„ë£Œ: {title[:20]}")

    return crawled_posts

def generate_html(all_posts, filename="dcinside_crawled_posts.html"):
    """
    ëˆ„ì ëœ ê²Œì‹œê¸€ ì •ë³´ë¥¼ HTML í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    ê° ê²Œì‹œê¸€ì€ ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€ ìˆœìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
    """
    html = """<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>DCInside í¬ë¡¤ë§ ê²°ê³¼</title>
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; }
        .post { border: 1px solid #ccc; margin: 20px 0; padding: 10px; }
        .post h2 { margin: 0; }
        .comments { margin-top: 10px; padding-left: 20px; }
    </style>
</head>
<body>
    <h1>DCInside í¬ë¡¤ë§ ê²°ê³¼</h1>
"""
    for post in all_posts:
        # ì¶”ì²œìˆ˜ê°€ ì €ì¥ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬
        recommend = post.get("recommend", 0)
        post_html = f"<div class='post'>\n"
        post_html += f"<h2>{post['title']} (ê²Œì‹œê¸€ ë²ˆí˜¸: {post['post_id']}, ì¶”ì²œìˆ˜: {recommend})</h2>\n"
        post_html += f"<div class='content'><p>{post['content'].replace(chr(10), '<br>')}</p></div>\n"
        if post.get("comments"):
            post_html += "<div class='comments'><h3>ëŒ“ê¸€:</h3><ul>\n"
            for comment in post["comments"]:
                post_html += f"<li>{comment}</li>\n"
            post_html += "</ul></div>\n"
        else:
            post_html += "<div class='comments'><p>ëŒ“ê¸€ ì—†ìŒ</p></div>\n"
        post_html += "</div>\n"
        html += post_html

    html += """
</body>
</html>
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"âœ… HTML ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(all_posts)}ê°œ ê²Œì‹œê¸€) â†’ {filename}")

def multiprocess_crawl(start_page, end_page, num_workers):
    """ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ í˜ì´ì§€ ë‹¨ìœ„ í¬ë¡¤ë§ í›„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    page_numbers = list(range(start_page, end_page + 1))
    all_posts = []
    with multiprocessing.Pool(num_workers) as pool:
        # ê° í˜ì´ì§€ì— ëŒ€í•´ fetch_dcinside_pageë¥¼ ì‹¤í–‰
        results = pool.map(fetch_dcinside_page, page_numbers)
    # ê²°ê³¼(ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)ë¥¼ í‰íƒ„í™”
    for page_posts in results:
        all_posts.extend(page_posts)
    return all_posts
        
if __name__ == "__main__":
    multiprocessing.freeze_support()  # Windowsì—ì„œ multiprocess ì•ˆì •í™”
    # ì›í•˜ëŠ” í˜ì´ì§€ ë²”ìœ„ ì„¤ì • (ì˜ˆ: 1í˜ì´ì§€ë¶€í„° 100í˜ì´ì§€ê¹Œì§€)
    start_page = 1
    end_page = 22643
    num_workers = 15
    all_posts = multiprocess_crawl(start_page, end_page, num_workers)
    html_filename = "dcinside_crawled_posts.html"
    generate_html(all_posts, html_filename)